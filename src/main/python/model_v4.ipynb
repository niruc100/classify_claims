{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Costum Classifier on Textual Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nickr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40608, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claims = pd.read_csv(\"../../../data/preprocessed_claims_new.csv\", index_col=0)\n",
    "claims.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx = claims[\"date\"].apply(lambda x : x.split(\"-\")[0]) != \"2022\"\n",
    "val_idx = claims[\"date\"].apply(lambda x : x.split(\"-\")[0]) == \"2022\"\n",
    "# double check\n",
    "\"2022\" in claims[train_idx][\"date\"].apply(lambda x: x.split(\"-\")[0]).value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37983,), (37983,), (2625,), (2625,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = claims[train_idx][\"claim\"].values\n",
    "y = claims[train_idx][\"truth_rating\"].values\n",
    "# validation data\n",
    "X_val = claims[val_idx][\"claim\"].values\n",
    "y_val = claims[val_idx][\"truth_rating\"].values\n",
    "X.shape, y.shape, X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Own models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_claims(claim, stopwords):\n",
    "    # works better than gensim's tokenize()\n",
    "    claim = word_tokenize(claim)\n",
    "    \n",
    "    filtered_claim = []\n",
    "    \n",
    "    for w in claim:\n",
    "        if w.lower() not in stopwords:\n",
    "            filtered_claim.append(w)\n",
    "    \n",
    "    return \" \".join(filtered_claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_tokenize(claim, stopwords):\n",
    "    \n",
    "    claim = word_tokenize(claim)\n",
    "    \n",
    "    filtered_claim = []\n",
    "    \n",
    "    for w in claim:\n",
    "        if w.lower() not in stopwords:\n",
    "            filtered_claim.append(w.lower())\n",
    "    \n",
    "    return filtered_claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class N_words_clf:\n",
    "    def __init__(self, top_N=100, stopwords=None):\n",
    "        self.N = top_N\n",
    "        self.stopwords = stopwords\n",
    "    \n",
    "    def clean_and_tokenize(self, claim):\n",
    "            \n",
    "        claim = word_tokenize(claim)\n",
    "        \n",
    "        if self.stopwords:\n",
    "            filtered_claim = []\n",
    "            for w in claim:\n",
    "                if w.lower() not in self.stopwords:\n",
    "                    filtered_claim.append(w.lower())\n",
    "            return filtered_claim\n",
    "        else:\n",
    "            return claim \n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        labels = np.unique(y_train)\n",
    "        profiles = {}\n",
    "        for label in labels:\n",
    "            subsample = X_train[y_train == label]\n",
    "            profile = pd.Series(np.hstack([self.clean_and_tokenize(claim) for claim in subsample])).value_counts().index\n",
    "            # save to profiles\n",
    "            profiles[label] = profile\n",
    "            \n",
    "        self.profiles = profiles\n",
    "                \n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        profiles = self.profiles\n",
    "        \n",
    "        predictions = []\n",
    "        for claim in tqdm(X_test):\n",
    "            claim_profile = word_tokenize(claim)\n",
    "            \n",
    "            best_score = -1\n",
    "            best_label = list(profiles.keys())[0]\n",
    "            \n",
    "            for label, profile in profiles.items():\n",
    "                score = np.array([w in profile[:self.N] for w in claim_profile]).sum()\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_label = label\n",
    "            \n",
    "            predictions.append(best_label)    \n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_w_stopwords = N_words_clf(top_N=10, stopwords=STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_w_stopwords.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2625/2625 [00:04<00:00, 636.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2625,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = clf_w_stopwords.predict(X_val)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2625/2625 [00:03<00:00, 794.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2625,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_wo_stopwords = N_words_clf(top_N = 10)\n",
    "clf_wo_stopwords.fit(X, y)\n",
    "predictions = clf_wo_stopwords.predict(X_val)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = [10, 100, 1000, 10000, 100000]\n",
    "for p in ps:\n",
    "    print(f\"WORD_MODEL with top_N = {p}\")\n",
    "    clf_wo_stopwords = N_words_clf(top_N=p)\n",
    "    clf_wo_stopwords.fit(X, y)\n",
    "    predictions = clf_wo_stopwords.predict(X_val)\n",
    "    print(classification_report(y_true=y_val, y_pred=predictions))\n",
    "    print(\"_\"*100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1680, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = pd.read_csv(\"../../../data/test_set.csv\", index_col=0)\n",
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_set[\"claim\"].values\n",
    "y_test = test_set[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_n_word_clf = N_words_clf(top_N=100)\n",
    "best_n_word_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1680/1680 [08:08<00:00,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       FALSE       0.44      0.92      0.60       700\n",
      "     NEITHER       0.52      0.11      0.18       679\n",
      "        TRUE       0.41      0.10      0.16       301\n",
      "\n",
      "    accuracy                           0.45      1680\n",
      "   macro avg       0.45      0.38      0.31      1680\n",
      "weighted avg       0.47      0.45      0.35      1680\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FALSE      1461\n",
       "NEITHER     145\n",
       "TRUE         74\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = best_n_word_clf\n",
    "test_predictions = clf.predict(X_test)\n",
    "final_predictions = np.array([\"NEITHER\" if l == \"OTHER\" else l for l in test_predictions])\n",
    "print(classification_report(y_true= y_test, y_pred=final_predictions))\n",
    "pd.Series(final_predictions).value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KDD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 | packaged by conda-forge | (main, Mar 30 2022, 08:38:02) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86d1cac61c1a8a4ca677cb2a2078730a65cbf222114e2ee6531c514518eb6f30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
